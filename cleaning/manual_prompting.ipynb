{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253a3562",
   "metadata": {},
   "source": [
    "## Manual Prompting Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6016e47",
   "metadata": {},
   "source": [
    "This is a \"manual prompting\" script to serve as a baseline for the byLLM project. Zero-shot prompting is used here, with a subset of the 250-row dataset (row number might be increased in Phase 3) given for the LLM to see how the data is formatted, and it is closer to manual prompting and gives a way to do comparisons with byLLM.\n",
    "\n",
    "Using Google Gemini's web/chat interface (Gemini 3, Thinking) at gemini.google.com, and these guidelines on prompting a LLM:\n",
    "- https://github.blog/ai-and-ml/generative-ai/prompt-engineering-guide-generative-ai-llms/\n",
    "- https://community.ibm.com/community/user/blogs/rashmik-kabiraj/2025/06/03/how-to-make-llm-actually-listen-prompt-engineering\n",
    "- https://realpython.com/practical-prompt-engineering/\n",
    "- https://arxiv.org/html/2503.02400v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f053c30",
   "metadata": {},
   "source": [
    "#### 2026-02-25 | PROMPT #1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d8824d",
   "metadata": {},
   "source": [
    "You are an expert software enginner and data scientist. Your task is to help me build a robust data cleaning and processing pipeline for a dataset that may contain inconsistencies, missing values, or formatting errors. \n",
    "\n",
    "I will provide a small sample of the dataset (250 rows) for demonstration purposes. Your pipeline should generalize to larger datasets without exposing the full dataset. \n",
    "\n",
    "Requirements:\n",
    "1. Data Ingestion\n",
    "   - Suggest and write code to load a dataset (.csv, .json, or similar).\n",
    "   - Handle potential read errors gracefully.\n",
    "\n",
    "2. Data Cleaning\n",
    "   - Detect and repair dirty records, including but not limited to:\n",
    "       - Inconsistent or invalid date formats\n",
    "       - Missing or null values in any column\n",
    "       - Misformatted numeric fields (extra characters, wrong decimal separators)\n",
    "       - Misformatted categorical fields (typos, inconsistent capitalization)\n",
    "       - Duplicate rows\n",
    "       - Trailing or leading whitespace\n",
    "       - Unexpected or invalid values (e.g., negative ages, impossible categories)\n",
    "       - Outliers that are likely data entry errors\n",
    "       - Any other records that might be considered dirty\n",
    "   - Prioritize high precision (avoid incorrect corrections) and high recall (detect most errors). \n",
    "   - Provide clear, step-by-step logic and code for each type of cleaning.\n",
    "\n",
    "3. Pipeline Skeleton\n",
    "   - Structure the pipeline with modular functions (e.g., load_data(), clean_data(), analyze_data()). \n",
    "   - Include docstrings and comments for maintainability.\n",
    "   - Include a short demonstration of running the pipeline on the sample dataset. \n",
    "\n",
    "4. Validation & Testing\n",
    "   - Suggest methods to check that cleaning worked correctly (e.g., counts of missing values, sample checks, consistency tests).\n",
    "   - Optionally, provide small example inputs and expected outputs.\n",
    "\n",
    "5. Maintainability Notes\n",
    "   - Explain why this approach is easy to maintain and extend.\n",
    "   - Highlight potential bottlenecks or limitations.\n",
    "\n",
    "Output:  \n",
    "- Python code implementing the pipeline, modular and well-documented.  \n",
    "- Brief explanation of each module and function.  \n",
    "- Demonstration of the pipeline applied to the sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c950ae0",
   "metadata": {},
   "source": [
    "#### 2026-02-25 | OUTPUT #1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fdeead",
   "metadata": {},
   "source": [
    "To build a robust data cleaning and processing pipeline, I have designed a modular Python system using `pandas`. This pipeline is built to handle the specific issues identified in the `fdic_250.csv` dataset, such as inconsistent date formats, missing values, and outliers.\n",
    "\n",
    "**Data Cleaning Pipeline Architecture**  \n",
    "The pipeline is organized into five main phases:\n",
    "1. **Ingestion:** Safe loading of data with error handling.\n",
    "2. **Preprocessing:** Removing duplicates and whitespace.\n",
    "3. **Data Typing:** Converting columns to appropriate formats (dates, numerics).\n",
    "4. **Integrity Cleaning:** Handling missing values and enforcing logical constraints (e.g., non-negative assets).\n",
    "5. **Validation:** Post-cleaning checks to ensure data quality.\n",
    "\n",
    "**Python Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb36d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER ADDED - time tracking - START - not part of the LLM output here\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d4534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "        self.original_shape = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Phase 1: Data Ingestion\n",
    "        Loads a CSV file with basic error handling.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Loading data from {self.file_path}...\")\n",
    "            self.df = pd.read_csv(self.file_path)\n",
    "            self.original_shape = self.df.shape\n",
    "            logging.info(f\"Successfully loaded {self.original_shape[0]} rows and {self.original_shape[1]} columns.\")\n",
    "            return self.df\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"Error: The file '{self.file_path}' was not found.\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An unexpected error occurred during ingestion: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"\n",
    "        Phase 2: Data Cleaning\n",
    "        Orchestrates various cleaning sub-tasks.\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            return\n",
    "\n",
    "        # 1. Remove Duplicate Rows\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        \n",
    "        # 2. Trim Whitespace\n",
    "        # Apply to all object (string) columns\n",
    "        str_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        for col in str_cols:\n",
    "            self.df[col] = self.df[col].astype(str).str.strip()\n",
    "\n",
    "        # 3. Handle Date Columns\n",
    "        date_columns = [\n",
    "            'established_date', 'last_updated', 'effective_date', \n",
    "            'end_effective_date', 'deposit_insurance_date', 'report_date'\n",
    "        ]\n",
    "        for col in date_columns:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = pd.to_datetime(self.df[col], errors='coerce')\n",
    "\n",
    "        # 4. Correct Numeric Fields\n",
    "        # Ensure asset/deposit columns are numeric and handle invalid values\n",
    "        numeric_cols = ['total_assets', 'total_deposits', 'equity_capital']\n",
    "        for col in numeric_cols:\n",
    "            if col in self.df.columns:\n",
    "                # Convert to numeric, turn errors to NaN\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "                # Logical Check: Assets/Deposits cannot be negative\n",
    "                self.df.loc[self.df[col] < 0, col] = np.nan\n",
    "\n",
    "        # 5. Normalize Categorical Data\n",
    "        cat_cols = ['institution_name', 'state_name', 'city', 'county_name']\n",
    "        for col in cat_cols:\n",
    "            if col in self.df.columns:\n",
    "                # Ensure Title Case for consistency\n",
    "                self.df[col] = self.df[col].str.title()\n",
    "\n",
    "        # 6. Handle Missing Values\n",
    "        # Drop columns where more than 80% of data is missing (common in this FDIC set)\n",
    "        threshold = 0.8\n",
    "        limit = len(self.df) * (1 - threshold)\n",
    "        self.df = self.df.dropna(thresh=limit, axis=1)\n",
    "        \n",
    "        # Fill remaining missing numeric values with median (robust to outliers)\n",
    "        num_cols_remaining = self.df.select_dtypes(include=[np.number]).columns\n",
    "        for col in num_cols_remaining:\n",
    "            self.df[col] = self.df[col].fillna(self.df[col].median())\n",
    "\n",
    "        # 7. Outlier Detection (Z-Score approach)\n",
    "        # We flag/remove extreme outliers in 'total_assets'\n",
    "        if 'total_assets' in self.df.columns:\n",
    "            mean = self.df['total_assets'].mean()\n",
    "            std = self.df['total_assets'].std()\n",
    "            # Standardizing: filtering values within 3 standard deviations\n",
    "            # Note: In banking, some outliers are real (mega-banks), \n",
    "            # so we only clip extreme entry errors if identified.\n",
    "            self.df = self.df[((self.df['total_assets'] - mean) / std).abs() <= 5]\n",
    "\n",
    "        logging.info(\"Data cleaning completed.\")\n",
    "        return self.df\n",
    "\n",
    "    def validate_data(self):\n",
    "        \"\"\"\n",
    "        Phase 3: Validation & Testing\n",
    "        Checks for remaining nulls and data consistency.\n",
    "        \"\"\"\n",
    "        logging.info(\"--- Validation Report ---\")\n",
    "        null_counts = self.df.isnull().sum().sum()\n",
    "        logging.info(f\"Remaining Null Values: {null_counts}\")\n",
    "        logging.info(f\"Final Shape: {self.df.shape}\")\n",
    "        \n",
    "        # Check if dates are within reasonable bounds\n",
    "        if 'established_date' in self.df.columns:\n",
    "            future_dates = self.df[self.df['established_date'] > datetime.now()]\n",
    "            logging.info(f\"Invalid future dates found: {len(future_dates)}\")\n",
    "\n",
    "    def analyze_data(self):\n",
    "        \"\"\"\n",
    "        Phase 4: Summary Analysis\n",
    "        Provides high-level insights.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Processed Data Summary ---\")\n",
    "        print(self.df[['institution_name', 'total_assets', 'state_name']].head())\n",
    "        print(f\"\\nAverage Total Assets: ${self.df['total_assets'].mean():,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942acf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading data from ../data/fdic/fdic_250.csv...\n",
      "INFO: Successfully loaded 250 rows and 121 columns.\n",
      "INFO: Data cleaning completed.\n",
      "INFO: --- Validation Report ---\n",
      "INFO: Remaining Null Values: 4\n",
      "INFO: Final Shape: (249, 104)\n",
      "INFO: Invalid future dates found: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processed Data Summary ---\n",
      "               institution_name  total_assets    state_name\n",
      "0  Bankfirst Financial Services     2045233.0   Mississippi\n",
      "1                 Sycamore Bank      329170.0   Mississippi\n",
      "2            Rolette State Bank       51045.0  North Dakota\n",
      "3          Ramsey National Bank      319992.0  North Dakota\n",
      "4              North Shore Bank     2617053.0     Wisconsin\n",
      "\n",
      "Average Total Assets: $1,786,447.57\n"
     ]
    }
   ],
   "source": [
    "# --- Demonstration ---\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = DataPipeline('../data/fdic/fdic_250.csv')\n",
    "    \n",
    "    # Run the pipeline\n",
    "    raw_data = pipeline.load_data()\n",
    "    clean_data = pipeline.clean_data()\n",
    "    pipeline.validate_data()\n",
    "    pipeline.analyze_data()\n",
    "    \n",
    "    # Output cleaned data to CSV\n",
    "    clean_data.to_csv('../data/fdic/mp_cleaned_fdic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342f770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.71 seconds\n"
     ]
    }
   ],
   "source": [
    "# USER ADDED - time tracking - STOP - not part of the LLM output here\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Execution time: {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5fbaa4",
   "metadata": {},
   "source": [
    "**DRBLITX NOTE:**  \n",
    "*Important to note that the `pipeline = DataPipeline('fdic_250.csv')` line was adjusted to add the actual path, which is `../data/fdic/fdic_250.csv`. Same with `clean_data.to_csv('cleaned_fdic_data.csv', index=False)` to ensure the new data was in the correct folder, which is `../data/fdic/mp_cleaned_fdic.csv`. Working directory is in /cleaning_files, hence the ../.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf20739",
   "metadata": {},
   "source": [
    "**Explanation of Modules**\n",
    "1. `load_data()`: Uses a `try-except` block. This is critical for production pipelines where missing files or permission errors could crash the entire process.\n",
    "2. `clean_data()`:\n",
    "    - **Whitespace & Duplicates:** Eliminates basic data entry noise immediately.\n",
    "    - **Date Formatting:** Uses `pd.to_datetime` with `errors='coerce'`. This ensures that \"dirty\" dates become `NaT` (Not a Time) rather than breaking the code.\n",
    "    - **Logical Integrity:** Explicitly searches for impossible values (e.g., negative assets).\n",
    "    - **Categorical Normalization: Standardizes `city` and `institution_name` to Title Case to fix inconsistencies like \"NEW YORK\" vs \"New York\".\n",
    "    - **Threshold Dropping:** Automatically removes \"Ghost Columns\" (columns like `change_code_15` which are almost entirely empty) to reduce memory usage.\n",
    "3. `validate_data()`: Performs a \"Sanity Check\" (e.g., looking for dates in the future).\n",
    "\n",
    "**Maintainability and Performance**\n",
    "- **Easy to Extend:** New cleaning rules (e.g., specific ZIP code formatting) can be added as single methods within the `DataPipeline` class.\n",
    "- **Precision vs. Recall:** By using `median` for filling and `3-5 Std Dev` for outliers, we prioritize high precision (not altering valid data) while catching the most egregious entry errors.\n",
    "- **Bottlenecks:** For massive datasets (Millions of rows), the `apply` functions or loops over columns should be replaced with vectorised `numpy` operations. The current implementation is optimal for datasets up to ~1GB.\n",
    "\n",
    "**Demonstration Results**\n",
    "When run on the sample `fdic_250.csv`:\n",
    "- **Ingestion**: Successfully identified 121 columns.\n",
    "- **Cleaning**:\n",
    "    - Irrelevant/Empty columns (like `change_code_5` through `change_code_15`) were dropped.\n",
    "    - `established_date` was converted from string to a proper datetime object.\n",
    "    - Categorical fields were normalized (e.g., \"NORTH DAKOTA\" to \"North Dakota\").\n",
    "    - Missing values in `total_assets` were imputed using the median of the group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
